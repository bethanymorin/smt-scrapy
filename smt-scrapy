#!/usr/bin/env python

import argparse
import subprocess

from scraper.utils import (
    write_all_node_data_from_db_to_file,
    write_urls_txt_file_from_node_data_json
)


def handle_crawl(args):

    crawl_stories()

    crawl_authors()

    # unique_urls = []
    # contributor_count = 0
    # article_count = 0

    # try:
    #     with open('output/stories.jl') as result_file:
    #         for line in result_file:
    #             line_data = json.loads(line)
    #             if line_data['url'] not in unique_urls:
    #                 unique_urls.append(line_data['url'])
    #                 article_count += 1
    # except Exception:
    #     print "couldn't open stories.jl"
    #     logging.error("couldn't open stories.jl")
    #
    # try:
    #     with open('output/authors.jl') as result_file:
    #         for line in result_file:
    #             line_data = json.loads(line)
    #             if line_data['url'] not in unique_urls:
    #                 unique_urls.append(line_data['url'])
    #                 contributor_count += 1
    # except Exception:
    #     print "couldn't open authors.jl"
    #     logging.error("couldn't open authors.jl")
    #
    # logging.info("%d unique urls scraped" % len(unique_urls))
    # logging.info("%d articles parsed" % article_count)
    # logging.info("%d profiles parsed" % contributor_count)
    # logging.info(" >>>>>>>>>>>>>>>>>> END\n")


def crawl_stories():
    first_command = 'scrapy crawl stories'

    process = subprocess.Popen(first_command, shell=True)
    process.wait()
    print process.returncode


def crawl_authors():
    second_command = 'scrapy crawl authors'

    process = subprocess.Popen(second_command, shell=True)
    process.wait()
    print process.returncode


def main():

    parser = argparse.ArgumentParser(
        description=(
            'smt-scrapy does all the smt-scrapy things you would ever want.'
        )
    )
    subparsers = parser.add_subparsers()

    db_parser = subparsers.add_parser('db', help='Do stuff with the database.')
    db_parser.set_defaults(func=write_all_node_data_from_db_to_file)

    crawl_parser = subparsers.add_parser('crawl', help='Do the crawling stuff.')
    crawl_parser.set_defaults(func=handle_crawl)

    make_urls_parser = subparsers.add_parser('urls', help='Make the urls.txt file of urls to crawl.')
    make_urls_parser.set_defaults(func=write_urls_txt_file_from_node_data_json)

    args = parser.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
